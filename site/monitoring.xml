<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!DOCTYPE html [
<!ENTITY % entities SYSTEM "rabbit.ent">
%entities;
]>
<!--
Copyright (c) 2007-2019 Pivotal Software, Inc.

All rights reserved. This program and the accompanying materials
are made available under the terms of the under the Apache License,
Version 2.0 (the "License”); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc">
  <head>
    <title>Monitoring</title>
  </head>
  <body show-in-this-page="true">
    <doc:section name="overview">
      <doc:heading>Overview</doc:heading>

      <p class="intro">
        This document provides an overview of topics related to RabbitMQ monitoring.
        Monitoring your RabbitMQ installation is an effective means to detect issues before they affect
        the rest of your environment and, eventually, your users.
      </p>

      <p>
        Many aspects of the system can be monitored. This guide will group them into a handful of
        categories:

        <ul>
          <li>
            <a href="#approaches-to-monitoring">What is monitoring</a>, what common approaches to it
            exist and why it is important.
          </li>
          <li>What <a href="#system-metrics">infrastructure and kernel metrics</a> are important to monitor</li>
          <li>
            What <a href="#rabbitmq-metrics">RabbitMQ metrics</a> are available:

            <ul>
              <li><a href="#node-metrics">Node metrics</a></li>
              <li><a href="#queue-metrics">Queue metrics</a></li>
              <li><a href="#cluster-wide-metrics">Cluster-wide metrics</a></li>
            </ul>
          </li>
          <li>
            <a href="#monitoring-frequency">How frequently</a> should monitoring checks be performed?
          </li>
          <li>
            <a href="#app-metrics">Application-level metrics</a>
          </li>
          <li>
            How to check a <a href="#health-checks">node's health</a> and why it's more involved than a single
            CLI command.
          </li>
        </ul>
      </p>

      <p>
        <a href="#log-collection">Log aggregation</a> across all nodes and applications is closely related to monitoring
        and also covered in this guide.
      </p>

      <p>
        A number of <a href="#monitoring-tools">popular tools</a>, both open source and commercial,
        can be used to monitor RabbitMQ.
      </p>
    </doc:section>


    <doc:section name="approaches-to-monitoring">
      <doc:heading>What is Monitoring?</doc:heading>

      <p>
        For the purpose of this guide we define monitoring as a continuous process of
        capturing the behaviour of a system via health checks and metrics in order to
        detect anomalies: when the system is unavailable, experiences an unusual load,
        exhausted of certain resources or otherwise does not behave within its normal
        (expected) parameters. Monitoring involves collecting and storing metrics for the long term,
        which is critically important not just for anomaly detection
        but also root cause analysis, trend detection and capacity planning.
      </p>

      <p>
        Monitoring systems typically integrate with alerting systems.

        When an anomaly is detected by a monitoring system an alarm of some sort is typically
        passed to an alerting system, which notifies interested parties such as the technical operations team.
      </p>

      <p>
        Having monitoring in place means that important deviations in system behavior,
        from degraded service in some areas to complete unavailability, is easier
        to detect and the root cause takes significantly less time to find.
        Operating a distributed system is a bit like trying to get out of a forest
        without a GPS navigator device or compass. It doesn't matter how brilliant or experienced
        the person is, having relevant information is critically important for
        a good outcome.
      </p>

      <p>
        A <a href="#health-checks">Health check</a> is the most basic aspect of monitoring.
        It involves a <a href="#monitoring-frequency">periodically executed</a> command
        or set of commands that collect a few essential metrics of the monitored system
        and evaluate them. For example, whether RabbitMQ's Erlang VM is running is one such
        check. It involves a metric (is an OS process running?), an expected range
        of normal operating parameters (the process is expected to be running, otherwise
        it cannot possibly serve any clients) and an evaluation step. Of course,
        there are more varieties of health checks. Which one is considered to be most appropriate
        depends on the definition of a "healthy node" used, and thus is a system- and team-specific decision.
        <a href="/cli.html">RabbitMQ CLI tools</a> provide a number of commands that can serve as useful health checks. They will
        be covered <a href="#health-checks">later in this guide</a>.
      </p>

      <p>
        While health checks are a useful tool, they only provide so much insight into
        the state of the system because they are by design focused on just one or a handful
        of metrics, usually check a single node and can only reason about the state of
        that node at a particular moment in time. For a more comprehensive assessment
        many more metrics have to be collected continuously at <a href="#monitoring-frequency">reasonable intervals</a>.
        This allows more types of anomalies to be detected, as some can only be identified over longer periods
        of time. This is usually done by tools commonly referred to as monitoring tools of
        which there are a grand variety. This guides covers some tools commonly
        used for RabbitMQ monitoring.
      </p>

      <p>
        Some metrics are RabbitMQ-specific: they are <a href="#rabbitmq-metrics">collected and reported by RabbitMQ nodes</a>.
        Rest of the guide refers to them as just "RabbitMQ metrics". Examples include the number of socket
        descriptors used, total number of enqueued messages or inter-node communication traffic
        rates. Others metrics are <a href="#system-metrics">collected and reported by the OS kernel</a>. Such
        metrics are often called system metrics or infrastructure metrics.
        System metrics are not RabbitMQ-specific: CPU utilisation rate, total amount of memory used by
        a certain process or group of processes, network packet loss rate and so on.
        Both types are important to monitor as individual metrics are not useful in
        all scenarios but when analysed collectively, they can quickly provide a reasonably
        complete insight into the state of the system, and help operators form a hypothesis
        as to what's going on and needs further investigation and/or fixing.
      </p>

      <doc:subsection name="monitoring-frequency">
        <doc:heading>Frequency of Monitoring</doc:heading>

        <p>
          Many monitoring systems poll their monitored services periodically. How often that's
          done varies from tool to tool but usually can be configured by the operator.
        </p>

        <p>
          Very frequent polling can have negative consequences on the system under monitoring.
          For example, excessive load balancer checks that open a test TCP connection to a node
          can lead to a <a href="/networking.html#dealing-with-high-connection-churn">high connection churn</a>.
          Excessive checks of channels and queues in RabbitMQ will increase its CPU consumption, which
          can be substantial when there are many (say, 10s of thousands) of them on a node.
        </p>

        <p>
          It is recommended that metrics are collected at 60 second intervals (or at least 30 second
          intervals if higher update rate is desired). More frequent collection will increase load and
          the system and provide no practical benefits for monitoring systems.
        </p>
      </doc:subsection>
    </doc:section>



    <doc:section name="system-metrics">
      <doc:heading>Infrastructure and Kernel Metrics</doc:heading>

      <p>
        First step towards a useful monitoring system starts with infrastructure and
        kernel metrics. There are quite a few of them but some are more important than others.
        The following metrics should be monitored on all nodes in a RabbitMQ cluster and, if possible,
        all instances that host RabbitMQ clients:

        <ul class="plain">
          <li>CPU stats (user, system, iowait &amp; idle percentages)</li>
          <li>Memory usage (used, buffered, cached &amp; free percentages)</li>
          <li><a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt">Virtual Memory</a> statistics (dirty page flushes, writeback volume)</li>
          <li>Disk I/O (operations &amp; amount of data transferred per unit time, time to service operations)</li>
          <li>Free disk space on the mount used for the <a href="/relocate.html">node data directory</a></li>
          <li>File descriptors used by <code>beam.smp</code> vs. <a href="/networking.html#open-file-handle-limit">max system limit</a></li>
          <li>TCP connections by state (<code>ESTABLISHED</code>, <code>CLOSE_WAIT</code>, <code>TIME_WAIT</code>)</li>
          <li>Network throughput (bytes received, bytes sent) &amp; maximum network throughput)</li>
          <li>Network latency (between all RabbitMQ nodes in a cluster as well as to/from clients)</li>
        </ul>
      </p>

      <p>
        There is no shortage of existing tools (such as Prometheus or Datadog) that collect infrastructure
        and kernel metrics, store and visualise them over periods of time.
      </p>
    </doc:section>


    <doc:section name="rabbitmq-metrics">
      <doc:heading>RabbitMQ Metrics</doc:heading>
      <p>
        The RabbitMQ <a href="management.html">management plugin</a> provides an API for
        accessing RabbitMQ metrics. The plugin will store up to one day's worth of
        metric data. Longer term monitoring should be accomplished with an <a href="#monitoring-tools">external tool</a>.
      </p>

      <p>
        This section will cover multiple RabbitMQ-specific aspects of monitoring.
      </p>

      <doc:subsection name="clusters">
        <doc:heading>Monitoring Clusters</doc:heading>

        <p>
          When monitoring clusters it is important to understand the guarantees provided by the
          HTTP API. In a clustered environment every node can serve metric endpoint requests.
          This means that it is possible to retrieve cluster-wide metrics from any node, assuming the node
          <a href="/management.html#clustering">is capable of contacting its peers</a>. That node
          will collect and aggregate data from its peers as needed before producing a response.
        </p>

        <p>
          Every node also can serve requests to endpoints that provide <a href="#node-metrics">node-specific metrics</a>
          for itself as well as other cluster nodes. Like <a href="#system-metrics">infrastructure and OS metrics</a>,
          node-specific metrics must be stored for each node individually. However, HTTP API requests
          can be issued to any node.
        </p>

        <p>
          As mentioned earlier, inter-node connectivity issues
          will <a href="/management.html#clustering">affect HTTP API behaviour</a>. It is therefore
          recommended that a random online node is chosen for monitoring requests, e.g. with the help of
          a load balancer or <a href="https://en.wikipedia.org/wiki/Round-robin_DNS">round-robin DNS</a>.
        </p>

        <p>
          Note that some endpoints (e.g. aliveness check) perform operations on the contacted node
          specifically; they are an exception, not the rule.
        </p>
      </doc:subsection>

      <doc:subsection name="cluster-wide-metrics">
        <doc:heading>Cluster-wide Metrics</doc:heading>

        <p>
          Cluster-wide metrics provide the highest possible level view of cluster state. Some (cluster links, detected network
          partitions) respresent node interaction, others are aggregates of certain metrics across all cluster members.
          They are complimentary to infrastructure and node metrics.
        </p>

        <p>
          <code>GET /api/overview</code> is the <a href="management.html#http-api">HTTP API</a> endpoint that returns cluster-wide metrics.
        </p>

        <p>
          <table>
            <thead>
              <tr><td>Metric</td><td>JSON field name</td></tr>
            </thead>
            <tbody>
              <tr>
                <td>Cluster name</td>
                <td><code>cluster_name</code></td>
              </tr>
              <tr>
                <td>Cluster-wide message rates</td>
                <td><code>message_stats</code></td>
              </tr>
              <tr>
                <td>Total number of connections</td>
                <td><code>object_totals.connections</code></td>
              </tr>
              <tr>
                <td>Total number of channels</td>
                <td><code>object_totals.channels</code></td>
              </tr>
              <tr>
                <td>Total number of queues</td>
                <td><code>object_totals.queues</code></td>
              </tr>
              <tr>
                <td>Total number of consumers</td>
                <td><code>object_totals.consumers</code></td>
              </tr>
              <tr>
                <td>Total number of messages (ready plus unacknowledged)</td>
                <td><code>queue_totals.messages</code></td>
              </tr>
              <tr>
                <td>Number of messages ready for delivery</td>
                <td><code>queue_totals.messages_ready</code></td>
              </tr>
              <tr>
                <td>Number of <a href="/confirms.html">unacknowledged</a> messages</td>
                <td><code>queue_totals.messages_unacknowledged</code></td>
              </tr>
              <tr>
                <td>Messages published recently</td>
                <td><code>message_stats.publish</code></td>
              </tr>
              <tr>
                <td>Message publish rate</td>
                <td><code>message_stats.publish_details.rate</code></td>
              </tr>
              <tr>
                <td>Messages delivered to consumers recently</td>
                <td><code>message_stats.deliver_get</code></td>
              </tr>
              <tr>
                <td>Message delivery rate</td>
                <td><code>message_stats.deliver_get.rate</code></td>
              </tr>
              <tr>
                <td>Other message stats</td>
                <td><code>message_stats.*</code> (see <a href="https://rawcdn.githack.com/rabbitmq/rabbitmq-management/master/priv/www/doc/stats.html">this document</a>)</td>
              </tr>
            </tbody>
          </table>
        </p>
      </doc:subsection>

      <doc:subsection name="node-metrics">
        <doc:heading>Node Metrics</doc:heading>
        <p>
          There are two  <a href="management.html#http-api">HTTP API</a> endpoints that provide access to node-specific metrics:

          <ul>
            <li><code>GET /api/nodes/{node}</code> returns stats for a single node</li>
            <li><code>GET /api/nodes</code> returns stats for all cluster members</li>
          </ul>

          The latter endpoint returns an array of objects. Monitoring tools that support (or can support)
          that as an input should prefer that endpoint since it redunces the number of requests that has
          to be issued. When that's not the case, the former endpoint can be used to retrieve stats for
          every cluster member in turn. That implies that the monitoring system is aware of the
          list of cluster members.
        </p>

        <p>
          Most of the metrics represent point-in-time absolute values. Some, however, represent activity over a recent period of time
          (for example, GC runs and bytes reclaimed). The latter metrics are primarily useful when compared to their
          previous values and historical mean/percentile values.
        </p>

        <table>
          <thead>
            <tr><td>Metric</td><td>JSON field name</td></tr>
          </thead>
          <tbody>
            <tr>
              <td>Total amount of <a href="/memory-use.html">memory used</a></td>
              <td><code>mem_used</code></td>
            </tr>
            <tr>
              <td>Memory usage high watermark</td>
              <td><code>mem_limit</code></td>
            </tr>
            <tr>
              <td>Is a <a href="/memory.html">memory alarm</a> in effect?</td>
              <td><code>mem_alarm</code></td>
            </tr>
            <tr>
              <td>Free disk space low watermark</td>
              <td><code>disk_free_limit</code></td>
            </tr>
            <tr>
              <td>Is a <a href="/disk-alarms.html">disk alarm</a> in effect?</td>
              <td><code>disk_free_alarm</code></td>
            </tr>
            <tr>
              <td><a href="/networking.html#open-file-handle-limit">File descriptors available</a></td>
              <td><code>fd_total</code></td>
            </tr>
            <tr>
              <td>File descriptors used</td>
              <td><code>fd_used</code></td>
            </tr>
            <tr>
              <td>File descriptor open attempts</td>
              <td><code>io_file_handle_open_attempt_count</code></td>
            </tr>
            <tr>
              <td>Sockets available</td>
              <td><code>sockets_total</code></td>
            </tr>
            <tr>
              <td>Sockets used</td>
              <td><code>sockets_used</code></td>
            </tr>
            <tr>
              <td>Message store disk reads</td>
              <td><code>message_stats.disk_reads</code></td>
            </tr>
            <tr>
              <td>Message store disk writes</td>
              <td><code>message_stats.disk_writes</code></td>
            </tr>
            <tr>
              <td>Inter-node communication links</td>
              <td>cluster_links</td>
            </tr>
            <tr>
              <td>GC runs</td>
              <td><code>gc_num</code></td>
            </tr>
            <tr>
              <td>Bytes reclaimed by GC</td>
              <td><code>gc_bytes_reclaimed</code></td>
            </tr>
            <tr>
              <td>Erlang process limit</td>
              <td><code>proc_total</code></td>
            </tr>
            <tr>
              <td>Erlang processes used</td>
              <td><code>proc_used</code></td>
            </tr>
            <tr>
              <td>Runtime run queue</td>
              <td><code>run_queue</code></td>
            </tr>
          </tbody>
        </table>
      </doc:subsection>

      <doc:subsection name="queue-metrics">
        <doc:heading>Individual Queue Metrics</doc:heading>
        <p>
          Individual queue metrics are made available through the <a href="management.html#http-api">HTTP API</a>
          via the <code>GET /api/queues/{vhost}/{qname}</code> endpoint.
        </p>

        <table>
          <thead>
            <tr><td>Metric</td><td>JSON field name</td></tr>
          </thead>
          <tbody>
            <tr>
              <td>Memory</td>
              <td><code>memory</code></td>
            </tr>
            <tr>
              <td>Total number of messages (ready plus unacknowledged)</td>
              <td><code>messages</code></td>
            </tr>
            <tr>
              <td>Number of messages ready for delivery</td>
              <td><code>messages_ready</code></td>
            </tr>
            <tr>
              <td>Number of <a href="/confirms.html">unacknowledged</a> messages</td>
              <td><code>messages_unacknowledged</code></td>
            </tr>
            <tr>
              <td>Messages published recently</td>
              <td><code>message_stats.publish</code></td>
            </tr>
            <tr>
              <td>Message publishing rate</td>
              <td><code>message_stats.publish_details.rate</code></td>
            </tr>
            <tr>
              <td>Messages delivered recently</td>
              <td><code>message_stats.deliver_get</code></td>
            </tr>
            <tr>
              <td>Message delivery rate</td>
              <td><code>message_stats.deliver_get.rate</code></td>
            </tr>
            <tr>
              <td>Other message stats</td>
              <td><code>message_stats.*</code> (see <a href="https://rawcdn.githack.com/rabbitmq/rabbitmq-management/master/priv/www/doc/stats.html">this document</a>)</td>
            </tr>
          </tbody>
        </table>
      </doc:subsection>
    </doc:section>


    <doc:section name="app-metrics">
      <doc:heading>Application-level Metrics</doc:heading>
      <p>
        A system that uses RabbitMQ, or any messaging-based system, is almost always distributed or can
        be treated as such. In such systems it is often not immediately obvious which component
        is problematic. Every single one of them, including applications, should be monitored and
        investigated.
      </p>
      <p>
       Some infrastructure-level and RabbitMQ metrics can demonstrate
       presence of an unusual system behaviour or issue but can't pin
       point the root cause. For example, it is easy to tell that a
       node is running out of disk space but not always easy to tell why.
       This is where application metrics come in: they can help identify
       a run-away publisher, a repeatedly failing consumer, a consumer that cannot
       keep up with the rate, even a downstream service that's experiencing a slowdown
       (e.g. a missing index in a database used by the consumers).
      </p>
      <p>
        Some client libraries (e.g. RabbitMQ Java client) and frameworks (e.g. Spring AMQP)
        provide means of registering metrics collectors or collect metrics out of the box.
        With others developers have to track metrics in their application code.
      </p>
      <p>
        What metrics applications track can be system-specific but some are relevant
        to most systems:

        <ul>
          <li>Connection opening rate</li>
          <li>Channel opening rate</li>
          <li>Connection failure (recovery) rate</li>
          <li>Publishing rate</li>
          <li>Delivery rate</li>
          <li>Positive delivery acknowledgement rate</li>
          <li>Negative delivery acknowledgement rate</li>
          <li>Mean/95th percentile delivery processing latency</li>
        </ul>
      </p>
    </doc:section>


    <doc:section name="health-checks">
      <doc:heading>Health Checks</doc:heading>
      <p>
        A health check is a <a href="#monitoring-frequency">periodically executed</a> command
        or set of commands that collect a few essential metrics of a RabbitMQ node or cluster.
        Just like with human or veterinary health checks, there's a variety of checks that
        can be performed and some are more intrusive than others. Different checks also have
        a different probability of reporting <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positives</a>
        (a scenario when a node is reported as unhealthy even when it is).
      </p>

      <p>
        Health checks therefore should be thought of as a range of options, starting with the most
        basic and virtually never producing false positives to increasingly more comprehensive,
        intrusive, and opinionated checks that have a probability of false positives that should be
        taken into account. Health checks can verify the state of an individual node or the entire cluster. The former
        kind is known as node health checks and the latter as cluster health checks.
      </p>

      <doc:subsection name="individual-checks">
        <doc:heading>Individual Node Checks</doc:heading>
        <p>
          This section covers several examples of node health check. They are organised in stages.
          Higher stages perform increasingly comprehensive and opinionated checks which have a higher probability of
          false positives. Some stages have dedicated RabbitMQ CLI tool commands, others
          can require additional tools.
        </p>

        <p>
          Note that while the health checks are ordered, a greater number does not necessarily indicate
          a "better" check.

          The health checks can be used selectively and combined.
          Unless noted otherwise, the checks should follow the same <a href="#monitoring-frequency">monitoring frequency</a> recommendation
          as metric collection.
        </p>

        <h4>Stage 1</h4>
        <p>
          The most basic check ensures that the runtime is running
          and (indirectly) that CLI tools can authenticate to it.

          Except for the CLI tool authentication
          part, the probability of false positives can be considered approaching 0
          except for upgrades and maintenance windows.

          <code>rabbitmq-diagnostics ping</code> performs this check:

<pre class="lang-bash">
rabbitmq-diagnostics ping -q
# => Ping succeeded
</pre>

          On Windows:

<pre class="lang-bash">
rabbitmq-diagnostics.bat -q ping
</pre>
        </p>

        <h4>Stage 2</h4>
        <p>
          A slightly more comprehensive check is executing

<code>rabbitmq-diagnostics status</code>:

          This effectively includes the stage 1 check plus retrieves some essential
          system information which is useful for other checks and should always be
          available if RabbitMQ is running on the node (see below).

<pre class="lang-bash">
rabbitmq-diagnostics -q status
# => [output elided for brevity]
</pre>

          On Windows:

<pre class="lang-bash">
rabbitmq-diagnostics.bat -q status
# => [output elided for brevity]
</pre>

          This is a common way of sanity checking a node.
          The probability of false positives can be considered approaching 0
          except for upgrades and maintenance windows.
        </p>

        <h4>Stage 3</h4>
        <p>
          Includes previous checks and also verifies that the RabbitMQ application is running
          (not stopped with <a href="/rabbitmqctl.8.html#stop_app"><code>rabbitmqctl stop_app</code></a>
          or the <a href="/partitions.html">Pause Minority partition handling strategy</a>)
          and there are no resource alarms:

<pre class="lang-bash">
rabbitmqctl eval 'rabbit_alarm:get_alarms() =:= [].' | grep -q "true" &amp; \
rabbitmq-diagnostics status -q | grep -A 30 "running_applications" | grep -q "rabbit,\"RabbitMQ\""
# if the check succeeded, exit code will be 0
</pre>

          The probability of false positives is low. Systems hovering around their
          <a href="/alarms.html">high runtime memory watermark</a> will have a high probability of false positives.
          During upgrades and maintenance windows can raise significantly.
        </p>

        <p>

          Specifically for memory alarms, the <code>GET /api/nodes/{node}/memory</code> HTTP API endpoint can be used for additional checks:

<pre class="lang-bash">
curl --silent -u guest:guest -X GET http://127.0.0.1:15672/api/nodes/rabbit@hostname/memory | python -m json.tool
# => {
# =>     "memory": {
# =>         "connection_readers": 24100480,
# =>         "connection_writers": 1452000,
# =>         "connection_channels": 3924000,
# =>         "connection_other": 79830276,
# =>         "queue_procs": 17642024,
# =>         "queue_slave_procs": 0,
# =>         "plugins": 63119396,
# =>         "other_proc": 18043684,
# =>         "metrics": 7272108,
# =>         "mgmt_db": 21422904,
# =>         "mnesia": 1650072,
# =>         "other_ets": 5368160,
# =>         "binary": 4933624,
# =>         "msg_index": 31632,
# =>         "code": 24006696,
# =>         "atom": 1172689,
# =>         "other_system": 26788975,
# =>         "allocated_unused": 82315584,
# =>         "reserved_unallocated": 0,
# =>         "strategy": "rss",
# =>         "total": {
# =>             "erlang": 300758720,
# =>             "rss": 342409216,
# =>             "allocated": 383074304
# =>         }
# =>     }
# => }
</pre>

          The <a href="/memory-use.html">breakdown information</a> it produces can be reduced down to a single value using <a href="https://stedolan.github.io/jq/manual/">jq</a>
          or similar tools:

<pre class="lang-bash">
curl --silent -u guest:guest -X GET http://127.0.0.1:15672/api/nodes/rabbit@hostname/memory | jq ".memory.total.allocated"
# => 397365248
</pre>

          <code>rabbitmq-diagnostics -q memory_breakdown</code> provides access to the same per category data
          and supports various units:

<pre class="lang-bash">
rabbitmq-diagnostics -q memory_breakdown --unit "MB"
# => connection_other: 50.18 mb (22.1%)
# => allocated_unused: 43.7058 mb (19.25%)
# => other_proc: 26.1082 mb (11.5%)
# => other_system: 26.0714 mb (11.48%)
# => connection_readers: 22.34 mb (9.84%)
# => code: 20.4311 mb (9.0%)
# => queue_procs: 17.687 mb (7.79%)
# => other_ets: 4.3429 mb (1.91%)
# => connection_writers: 4.068 mb (1.79%)
# => connection_channels: 4.012 mb (1.77%)
# => metrics: 3.3802 mb (1.49%)
# => binary: 1.992 mb (0.88%)
# => mnesia: 1.6292 mb (0.72%)
# => atom: 1.0826 mb (0.48%)
# => msg_index: 0.0317 mb (0.01%)
# => plugins: 0.0119 mb (0.01%)
# => queue_slave_procs: 0.0 mb (0.0%)
# => mgmt_db: 0.0 mb (0.0%)
# => reserved_unallocated: 0.0 mb (0.0%)
</pre>
        </p>

        <h4>Stage 4</h4>
        <p>
          Includes all checks in stage 3 plus checks that there are no failed <a href="/vhosts.html">virtual hosts</a>.

          RabbitMQ CLI tools currently do not provide a dedicated command for this check.

          The probability of false positives is generally low except for systems that are under
          high CPU load.
        </p>

        <h4>Stage 5</h4>
        <p>
          Includes all checks in stage 4 plus a check on all enabled listeners
          (using a temporary TCP connection).

          RabbitMQ CLI tools currently do not provide a dedicated command for this check.

          The probability of false positives is generally low but during upgrades and
          maintenance windows can raise significantly.
        </p>

        <h4>Stage 6</h4>
        <p>
          Includes all checks in stage 5 plus checks all channel and queue processes
          on the target queue for aliveness.

          <code>rabbitmqctl node_health_check</code> is the closest command
          currently available to this check. The command includes all checks
          up to and including stage 3 and checks all channel and queue processes
          on the target queue for aliveness:

<pre class="lang-bash">
rabbitmq-diagnostics -q node_health_check
# => Health check passed
</pre>

          On Windows:

<pre class="lang-bash">
rabbitmq-diagnostics.bat -q node_health_check
# => Health check passed
</pre>

          The probability of false positives is moderate for systems under
          above average load or with a large number of queues and channels
          (starting with 10s of thousands).
        </p>

        <h4>Optional Check 1</h4>
        <p>
          This check verifies that an expected set of plugins is enabled. It is orthogonal to
          the primary checks.

          <a href="/rabbitmq-plugins.8.html#list">rabbitmq-plugins list --enabled</a> is the command that lists enabled plugins
          on a node:

<pre class="lang-bash">
rabbitmq-plugins -q list --enabled
# => Configured: E = explicitly enabled; e = implicitly enabled
# => | Status: * = running on rabbit@mercurio
# => |/
# => [E*] rabbitmq_auth_mechanism_ssl       3.7.10
# => [E*] rabbitmq_consistent_hash_exchange 3.7.10
# => [E*] rabbitmq_management               3.7.10
# => [E*] rabbitmq_management_agent         3.7.10
# => [E*] rabbitmq_shovel                   3.7.10
# => [E*] rabbitmq_shovel_management        3.7.10
# => [E*] rabbitmq_top                      3.7.10
# => [E*] rabbitmq_tracing                  3.7.10
</pre>

          A health check that verifies that a specific plugin, <a href="/shovel.html"><code>rabbitmq_shovel</code></a>
          is enabled and running:

<pre class="lang-bash">
rabbitmq-plugins -q list --enabled | grep -i "\brabbitmq_shovel\b" -q
# if the check succeeded, exit code will be 0
</pre>

          The probability of false positives is generally low but raises
          in environments where environment variables that can affect <a href="/cli.html">rabbitmq-plugins</a>
          are overridden.
        </p>
      </doc:subsection>
    </doc:section>


    <doc:section name="monitoring-tools">
      <doc:heading>Popular Monitoring Tools</doc:heading>
      <p>
        The following is an alphabetised list of third-party tools commonly used to collect RabbitMQ metrics. These
        tools vary in capabilities but usually can collect both <a href="system-metrics">infrastructure-level</a> and <a href="#rabbitmq-metrics">RabbitMQ metrics</a>.
        Note that this list is by no means complete.
      </p>
      <p>
        <table>
          <thead>
            <tr><td>Monitoring Tool</td><td>Online Resource(s)</td></tr>
          </thead>
          <tbody>
            <tr>
              <td>AppDynamics</td>
              <td><a href="https://www.appdynamics.com/community/exchange/extension/rabbitmq-monitoring-extension/">AppDynamics</a>, <a href="https://github.com/Appdynamics/rabbitmq-monitoring-extension">GitHub</a></td>
            </tr>
            <tr>
              <td><code>collectd</code></td>
              <td><a href="https://github.com/signalfx/integrations/tree/master/collectd-rabbitmq">GitHub</a></td>
            </tr>
            <tr>
              <td>DataDog</td>
              <td><a href="https://docs.datadoghq.com/integrations/rabbitmq/">DataDog RabbitMQ integration</a>, <a href="https://github.com/DataDog/integrations-core/tree/master/rabbitmq">GitHub</a></td>
            </tr>
            <tr>
              <td>Ganglia</td>
              <td><a href="https://github.com/ganglia/gmond_python_modules/tree/master/rabbit">GitHub</a></td>
            </tr>
            <tr>
              <td>Graphite</td>
              <td><a href="http://graphite.readthedocs.io/en/latest/tools.html">Tools that work with Graphite</a></td>
            </tr>
            <tr>
              <td>Munin</td>
              <td><a href="http://munin-monitoring.org/">Munin docs</a>, <a href="https://github.com/ask/rabbitmq-munin">GitHub</a></td>
            </tr>
            <tr>
              <td>Nagios</td>
              <td><a href="https://github.com/nagios-plugins-rabbitmq/nagios-plugins-rabbitmq">GitHub</a></td>
            </tr>
            <tr>
              <td>New Relic</td>
              <td><a href="https://newrelic.com/plugins/vmware-29/95">NewRelic Plugins</a>, <a href="https://github.com/pivotalsoftware/newrelic_pivotal_agent">GitHub</a></td>
            </tr>
            <tr>
              <td>Prometheus</td>
              <td><a href="/prometheus.html">Prometheus guide</a>, <a href="https://github.com/deadtrickster/prometheus_rabbitmq_exporter">GitHub</a></td>
            </tr>
            <tr>
              <td>Zabbix</td>
              <td><a href="http://blog.thomasvandoren.com/monitoring-rabbitmq-queues-with-zabbix.html">Blog article</a></td>
            </tr>
            <tr>
              <td>Zenoss</td>
              <td><a href="https://www.zenoss.com/product/zenpacks/rabbitmq">RabbitMQ ZenPack</a>, <a href="http://www.youtube.com/watch?v=CAak2ayFcV0">Instructional Video</a></td>
            </tr>
          </tbody>
        </table>
      </p>
    </doc:section>


    <doc:section name="log-collection">
      <doc:heading>Log Aggregation</doc:heading>
      <p>
        While not technically a metric, one more piece of information can be very useful
        in troubleshooting a multi-service distributed system: logs. Consider collecting logs
        from all RabbitMQ nodes as well as all applications (if possible). Like metrics,
        logs can provide important clues that will help identify the root cause.
      </p>
    </doc:section>
  </body>
</html>
